# Kubernetes Architecture

**How WatchingCat works in Kubernetes**

---

## Overview

WatchingCat uses a two-tier OpenTelemetry Collector architecture inspired by SigNoz K8s-Infra:

1. **otelAgent** (DaemonSet) - Runs on every node
2. **otelDeployment** (Deployment) - Cluster-level collection

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kubernetes Cluster                        â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ otelAgent (DaemonSet - every node)              â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ Collects:                                        â”‚ â”‚
â”‚  â”‚  â€¢ Host metrics (CPU, memory, disk, network)    â”‚ â”‚
â”‚  â”‚  â€¢ Kubelet metrics (pod/container resources)    â”‚ â”‚
â”‚  â”‚  â€¢ Container logs (Docker/Containerd/CRI-O)     â”‚ â”‚
â”‚  â”‚  â€¢ Application traces (OTLP/Jaeger/Zipkin)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ otelDeployment (Deployment - cluster-wide)      â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ Collects:                                        â”‚ â”‚
â”‚  â”‚  â€¢ Cluster metrics (API server, kube-state)     â”‚ â”‚
â”‚  â”‚  â€¢ Kubernetes events                             â”‚ â”‚
â”‚  â”‚  â€¢ Prometheus scraping (annotated pods)         â”‚ â”‚
â”‚  â”‚  â€¢ OTLP gateway (receives from agents)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          WatchingCat Backend (Deployment)              â”‚
â”‚  â€¢ Processes all telemetry                            â”‚
â”‚  â€¢ Unified REST API (port 8090)                       â”‚
â”‚  â€¢ Integrates with storage backends                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“          â†“          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Jaeger  â”‚ â”‚Prometheusâ”‚ â”‚Elasticsearch  â”‚
        â”‚(Traces) â”‚ â”‚(Metrics) â”‚ â”‚   (Logs)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Components

### otelAgent (DaemonSet)

**Why DaemonSet?**
- Needs to run on EVERY node
- Accesses host filesystem for logs
- Direct access to kubelet API
- Collects node-specific metrics

**What it collects:**
- âœ… Host metrics (CPU, memory, disk I/O, network)
- âœ… Kubelet metrics (pod/container resource usage)
- âœ… Container logs from `/var/log/pods`
- âœ… Application traces (OTLP, Jaeger, Zipkin receivers)

**Ports:**
- 4317 (OTLP gRPC)
- 4318 (OTLP HTTP)
- 14250 (Jaeger gRPC)
- 14268 (Jaeger HTTP)
- 9411 (Zipkin)
- 8888 (Prometheus metrics)

### otelDeployment (Deployment)

**Why Deployment?**
- Only needs one instance (or replicas for HA)
- Collects cluster-level data (no per-node duplication)
- API server access for cluster metrics
- Centralized Prometheus scraping

**What it collects:**
- âœ… Cluster metrics (deployments, pods, nodes)
- âœ… Kubernetes events
- âœ… Prometheus scraping from annotated pods
- âœ… Acts as OTLP gateway for aggregation

**Ports:**
- 4317 (OTLP gRPC gateway)
- 4318 (OTLP HTTP gateway)
- 8888 (Prometheus metrics)

---

## Data Flow

### 1. Application â†’ otelAgent

```
Application (instrumented) 
  â†’ OTLP/Jaeger/Zipkin
    â†’ otelAgent (running on same node)
```

### 2. otelAgent â†’ Backend

```
otelAgent 
  â†’ Processes & enriches with K8s metadata
    â†’ Sends via OTLP to WatchingCat Backend
```

### 3. Cluster â†’ otelDeployment

```
Kubernetes API Server
  â†’ k8s_cluster receiver
    â†’ otelDeployment
```

### 4. Backend â†’ Storage

```
WatchingCat Backend
  â”œâ†’ Jaeger (traces)
  â”œâ†’ Prometheus (metrics)
  â””â†’ Elasticsearch (logs)
```

---

## Why Two Collectors?

### Problem: Duplication

If only DaemonSet:
- Cluster metrics collected N times (once per node)
- Events collected N times
- Wastes resources and causes duplicates

### Solution: Two-Tier Architecture

- **DaemonSet** (otelAgent): Node-specific data
- **Deployment** (otelDeployment): Cluster-wide data

**Result**: No duplication, efficient resource usage

---

## Metadata Enrichment

All telemetry is automatically enriched with:

```yaml
k8s.namespace.name: "default"
k8s.pod.name: "my-app-12345"
k8s.pod.uid: "abc-def-ghi"
k8s.node.name: "node-1"
k8s.deployment.name: "my-app"
k8s.container.name: "app"
cluster.name: "production"
cloud.region: "us-west-2"
```

---

## Resource Requirements

### Per Node (otelAgent)
- CPU: 200m (request), 500m (limit)
- Memory: 256Mi (request), 512Mi (limit)

### Cluster-wide (otelDeployment)
- CPU: 500m (request), 1000m (limit)
- Memory: 512Mi (request), 1Gi (limit)

### Backend (per replica)
- CPU: 500m (request), 1000m (limit)
- Memory: 512Mi (request), 1Gi (limit)

**Total for 3-node cluster:**
- CPU: ~4-8 cores
- Memory: ~4-8 GB

---

## Security

### RBAC

**otelAgent permissions:**
- Read: nodes, pods, services, endpoints
- Read: deployments, daemonsets, statefulsets
- Access: /metrics endpoint

**otelDeployment permissions:**
- Read: all namespace resources
- Read: cluster-level resources
- Read: events

**No write permissions** - security-first!

### Pod Security

```yaml
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop: [ALL]
```

---

## Scalability

### Horizontal Scaling

**otelAgent:**
- Automatically scales with nodes (DaemonSet)

**otelDeployment:**
- Can scale replicas for high load
- Use HPA based on CPU/memory

**Backend:**
- Scale replicas based on load
- Use LoadBalancer service

### Vertical Scaling

Increase resources in `values.yaml`:

```yaml
otelAgent:
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
```

---

## Monitoring the Monitors

### Collector Metrics

Both collectors expose Prometheus metrics on port 8888:

```bash
# Check otelAgent metrics
kubectl exec -n observability watchingcat-otel-agent-xxxxx -- \
  curl localhost:8888/metrics
```

### Health Checks

```bash
# Check backend health
kubectl exec -n observability watchingcat-backend-xxxxx -- \
  curl localhost:8090/health
```

---

## Next Steps

- ğŸ“– [Helm Chart Guide](helm-chart.md) - Configure the chart
- ğŸš€ [Quick Start](quickstart.md) - Deploy to K8s
- âš™ï¸ [Production Guide](production.md) - Production setup

---

**Last Updated**: December 5, 2025

